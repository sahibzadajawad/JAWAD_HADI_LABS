{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Administrator\\\\Downloads\\\\ML Lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from timeseires.utils.to_split import to_split\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from timeseires.utils.multivariate_single_step import multivariate_single_step\n",
    "from timeseires.utils.univariate_multi_step import univariate_multi_step\n",
    "from timeseires.utils.univariate_single_step import univariate_single_step\n",
    "from timeseires.utils.CosineAnnealingLRS import CosineAnnealingLRS\n",
    "from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookback = 24\n",
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(time_steps , num_features)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 504)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                16160     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,193\n",
      "Trainable params: 16,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = MLP()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.keras.utils.plot_model(model1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = r'C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = r'C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"\\history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"\\history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model =MLP()\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\Time1\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84907, 21), (24259, 21), (12130, 21))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path_dataset ='C:\\\\Users\\\\Administrator\\\\Downloads\\\\ML Lab\\\\AEP_hourly\\\\processed'\n",
    "path_tr = os.path.join(path_dataset, 'AEP_train.csv')\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values\n",
    "path_v = os.path.join(path_dataset, 'AEP_validation.csv')\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "path_te = os.path.join(path_dataset, 'AEP_test.csv')\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "\n",
    "path_scaler = os.path.join(path_dataset, 'AEP_Scaler.pkl')\n",
    "scaler         = pickle.load(open(path_scaler, 'rb'))\n",
    "\n",
    "train_set.shape, validation_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aep</th>\n",
       "      <th>Is_holiday1</th>\n",
       "      <th>Is_holiday2</th>\n",
       "      <th>Is_Weekend1</th>\n",
       "      <th>Is_Weekend2</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_week</th>\n",
       "      <th>cos_week</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>...</th>\n",
       "      <th>sin_wintert</th>\n",
       "      <th>cos_wintert</th>\n",
       "      <th>sin_springt</th>\n",
       "      <th>cos_springt</th>\n",
       "      <th>sin_summert</th>\n",
       "      <th>cos_summert</th>\n",
       "      <th>sin_fallt</th>\n",
       "      <th>cos_fallt</th>\n",
       "      <th>sin_year_dayt</th>\n",
       "      <th>cos_year_dayt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.518532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956235</td>\n",
       "      <td>0.2926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aep  Is_holiday1  Is_holiday2  Is_Weekend1  Is_Weekend2  sin_month  \\\n",
       "0  0.518532          1.0          0.0          1.0          0.0        1.0   \n",
       "\n",
       "      cos_month  sin_week  cos_week  sin_hour  ...  sin_wintert  cos_wintert  \\\n",
       "0  6.123234e-17  0.866025      -0.5 -0.707107  ...          0.0          1.0   \n",
       "\n",
       "   sin_springt   cos_springt  sin_summert  cos_summert  sin_fallt  cos_fallt  \\\n",
       "0          1.0  6.123234e-17          0.0          1.0        0.0        1.0   \n",
       "\n",
       "   sin_year_dayt  cos_year_dayt  \n",
       "0       0.956235         0.2926  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_te.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.18532385e-01,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  1.00000000e+00,  6.12323400e-17,  8.66025404e-01,\n",
       "       -5.00000000e-01, -7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  6.12323400e-17,  0.00000000e+00,\n",
       "        1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  9.56234827e-01,\n",
       "        2.92600336e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=2\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=0,target_len=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.69100212e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         2.58819045e-01,  9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02],\n",
       "       [ 1.41395233e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         5.00000000e-01,  8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12323400e-17, -9.99768502e-01,  2.15160974e-02]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12623237, 0.12030451])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps =2\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=1,target_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.69100212e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         2.58819045e-01,  9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 1.41395233e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         5.00000000e-01,  8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 1.26232372e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         7.07106781e-01,  7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 1.20304505e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         8.66025404e-01,  5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 1.25545988e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         9.65925826e-01,  2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 1.62922751e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         1.00000000e+00,  6.12000000e-17,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 2.51029577e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         9.65925826e-01, -2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.08810683e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         8.66025404e-01, -5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.26594284e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.40009984e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         5.00000000e-01, -8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.47747410e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         2.58819045e-01, -9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.56046425e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "         1.22000000e-16, -1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.57855984e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -2.58819045e-01, -9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.73518033e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -5.00000000e-01, -8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.78759516e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -7.07106781e-01, -7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.78759516e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -8.66025404e-01, -5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.72831649e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -9.65925826e-01, -2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.54673655e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -1.00000000e+00, -1.84000000e-16,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.34768501e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -9.65925826e-01,  2.58819045e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.45813054e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -8.66025404e-01,  5.00000000e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.54424061e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -7.07106781e-01,  7.07106781e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 3.22475977e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -5.00000000e-01,  8.66025404e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 2.74429053e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01, -5.00000000e-01,\n",
       "        -2.58819045e-01,  9.65925826e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99768502e-01,  2.15160970e-02],\n",
       "       [ 2.17022339e-01,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00, -8.66025404e-01,\n",
       "         5.00000000e-01, -8.66025404e-01,  5.00000000e-01,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         6.12000000e-17, -9.99250011e-01,  3.87222810e-02]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16167478])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.9597721099853516 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=0,target_len=1)\n",
    "validation_X, validation_y = univariate_multi_step(validation_set, time_steps, target_col=0,target_len=1)\n",
    "test_X, test_y = univariate_multi_step(test_set, time_steps, target_col=0,target_len=1)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "   1/2653 [..............................] - ETA: 39:33 - loss: 0.9225 - mae: 0.9225 - mape: 251.7883WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.\n",
      "2651/2653 [============================>.] - ETA: 0s - loss: 0.0425 - mae: 0.0425 - mape: 1830.0505\n",
      "Epoch 1: val_loss improved from inf to 0.02679, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0001-loss0.03.h5\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0425 - mae: 0.0425 - mape: 1828.9777 - val_loss: 0.0268 - val_mae: 0.0268 - val_mape: 11.0248\n",
      "Epoch 2/60\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0246 - mape: 765.9785\n",
      "Epoch 2: val_loss improved from 0.02679 to 0.02149, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0002-loss0.02.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0246 - mae: 0.0246 - mape: 765.8170 - val_loss: 0.0215 - val_mae: 0.0215 - val_mape: 9.6927\n",
      "Epoch 3/60\n",
      "2651/2653 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0213 - mape: 376.4086\n",
      "Epoch 3: val_loss improved from 0.02149 to 0.01947, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0003-loss0.02.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0213 - mae: 0.0213 - mape: 376.1910 - val_loss: 0.0195 - val_mae: 0.0195 - val_mape: 8.1515\n",
      "Epoch 4/60\n",
      "2650/2653 [============================>.] - ETA: 0s - loss: 0.0192 - mae: 0.0192 - mape: 619.2632\n",
      "Epoch 4: val_loss improved from 0.01947 to 0.01923, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0004-loss0.02.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0192 - mae: 0.0192 - mape: 618.6714 - val_loss: 0.0192 - val_mae: 0.0192 - val_mape: 9.3209\n",
      "Epoch 5/60\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0176 - mae: 0.0176 - mape: 182.7170\n",
      "Epoch 5: val_loss did not improve from 0.01923\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0176 - mae: 0.0176 - mape: 182.7170 - val_loss: 0.0201 - val_mae: 0.0201 - val_mape: 9.3119\n",
      "Epoch 6/60\n",
      "2637/2653 [============================>.] - ETA: 0s - loss: 0.0168 - mae: 0.0168 - mape: 255.7528\n",
      "Epoch 6: val_loss improved from 0.01923 to 0.01663, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0006-loss0.02.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0168 - mae: 0.0168 - mape: 254.2827 - val_loss: 0.0166 - val_mae: 0.0166 - val_mape: 7.5221\n",
      "Epoch 7/60\n",
      "2626/2653 [============================>.] - ETA: 0s - loss: 0.0161 - mae: 0.0161 - mape: 496.1837\n",
      "Epoch 7: val_loss improved from 0.01663 to 0.01615, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0007-loss0.02.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0161 - mae: 0.0161 - mape: 491.2614 - val_loss: 0.0162 - val_mae: 0.0162 - val_mape: 7.2634\n",
      "Epoch 8/60\n",
      "2641/2653 [============================>.] - ETA: 0s - loss: 0.0156 - mae: 0.0156 - mape: 138.6219\n",
      "Epoch 8: val_loss improved from 0.01615 to 0.01398, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0008-loss0.01.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0156 - mae: 0.0156 - mape: 138.0380 - val_loss: 0.0140 - val_mae: 0.0140 - val_mape: 6.1484\n",
      "Epoch 9/60\n",
      "2648/2653 [============================>.] - ETA: 0s - loss: 0.0151 - mae: 0.0151 - mape: 63.0054\n",
      "Epoch 9: val_loss did not improve from 0.01398\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0151 - mae: 0.0151 - mape: 62.9048 - val_loss: 0.0171 - val_mae: 0.0171 - val_mape: 8.3438\n",
      "Epoch 10/60\n",
      "2634/2653 [============================>.] - ETA: 0s - loss: 0.0148 - mae: 0.0148 - mape: 275.7564\n",
      "Epoch 10: val_loss did not improve from 0.01398\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0148 - mae: 0.0148 - mape: 273.8574 - val_loss: 0.0161 - val_mae: 0.0161 - val_mape: 7.3658\n",
      "Epoch 11/60\n",
      "2631/2653 [============================>.] - ETA: 0s - loss: 0.0146 - mae: 0.0146 - mape: 188.2150\n",
      "Epoch 11: val_loss improved from 0.01398 to 0.01312, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0011-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0146 - mae: 0.0146 - mape: 186.7218 - val_loss: 0.0131 - val_mae: 0.0131 - val_mape: 5.5736\n",
      "Epoch 12/60\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0142 - mae: 0.0142 - mape: 185.0725\n",
      "Epoch 12: val_loss did not improve from 0.01312\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0142 - mae: 0.0142 - mape: 185.0341 - val_loss: 0.0147 - val_mae: 0.0147 - val_mape: 5.9098\n",
      "Epoch 13/60\n",
      "2644/2653 [============================>.] - ETA: 0s - loss: 0.0138 - mae: 0.0138 - mape: 97.8355\n",
      "Epoch 13: val_loss improved from 0.01312 to 0.01288, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0013-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0138 - mae: 0.0138 - mape: 97.5363 - val_loss: 0.0129 - val_mae: 0.0129 - val_mape: 5.7404\n",
      "Epoch 14/60\n",
      "2650/2653 [============================>.] - ETA: 0s - loss: 0.0138 - mae: 0.0138 - mape: 324.3739\n",
      "Epoch 14: val_loss did not improve from 0.01288\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0138 - mae: 0.0138 - mape: 324.0647 - val_loss: 0.0174 - val_mae: 0.0174 - val_mape: 8.6093\n",
      "Epoch 15/60\n",
      "2650/2653 [============================>.] - ETA: 0s - loss: 0.0137 - mae: 0.0137 - mape: 332.8695\n",
      "Epoch 15: val_loss did not improve from 0.01288\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0137 - mae: 0.0137 - mape: 332.5524 - val_loss: 0.0152 - val_mae: 0.0152 - val_mape: 6.9027\n",
      "Epoch 16/60\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0135 - mae: 0.0135 - mape: 271.9108\n",
      "Epoch 16: val_loss did not improve from 0.01288\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0135 - mae: 0.0135 - mape: 271.9108 - val_loss: 0.0138 - val_mae: 0.0138 - val_mape: 5.7036\n",
      "Epoch 17/60\n",
      "2625/2653 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0133 - mape: 342.7396\n",
      "Epoch 17: val_loss did not improve from 0.01288\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0133 - mae: 0.0133 - mape: 339.2264 - val_loss: 0.0139 - val_mae: 0.0139 - val_mape: 7.0975\n",
      "Epoch 18/60\n",
      "2629/2653 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0130 - mape: 179.1804\n",
      "Epoch 18: val_loss did not improve from 0.01288\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0130 - mae: 0.0130 - mape: 177.6293 - val_loss: 0.0131 - val_mae: 0.0131 - val_mape: 5.7707\n",
      "Epoch 19/60\n",
      "2633/2653 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0132 - mape: 474.7893\n",
      "Epoch 19: val_loss did not improve from 0.01288\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0132 - mae: 0.0132 - mape: 471.3138 - val_loss: 0.0137 - val_mae: 0.0137 - val_mape: 6.0983\n",
      "Epoch 20/60\n",
      "2646/2653 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0130 - mape: 330.9679\n",
      "Epoch 20: val_loss did not improve from 0.01288\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0130 - mae: 0.0130 - mape: 330.1640 - val_loss: 0.0145 - val_mae: 0.0145 - val_mape: 6.7434\n",
      "Epoch 21/60\n",
      "2641/2653 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0129 - mape: 225.9091\n",
      "Epoch 21: val_loss improved from 0.01288 to 0.01268, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0021-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0129 - mae: 0.0129 - mape: 224.9412 - val_loss: 0.0127 - val_mae: 0.0127 - val_mape: 6.3007\n",
      "Epoch 22/60\n",
      "2651/2653 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0129 - mape: 299.0386\n",
      "Epoch 22: val_loss improved from 0.01268 to 0.01166, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0022-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0129 - mae: 0.0129 - mape: 298.8647 - val_loss: 0.0117 - val_mae: 0.0117 - val_mape: 5.1540\n",
      "Epoch 23/60\n",
      "2628/2653 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0128 - mape: 401.5852\n",
      "Epoch 23: val_loss did not improve from 0.01166\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0128 - mae: 0.0128 - mape: 397.9055 - val_loss: 0.0138 - val_mae: 0.0138 - val_mape: 6.7560\n",
      "Epoch 24/60\n",
      "2630/2653 [============================>.] - ETA: 0s - loss: 0.0125 - mae: 0.0125 - mape: 149.4686\n",
      "Epoch 24: val_loss did not improve from 0.01166\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0125 - mae: 0.0125 - mape: 148.2322 - val_loss: 0.0129 - val_mae: 0.0129 - val_mape: 6.6409\n",
      "Epoch 25/60\n",
      "2649/2653 [============================>.] - ETA: 0s - loss: 0.0125 - mae: 0.0125 - mape: 18.8278\n",
      "Epoch 25: val_loss did not improve from 0.01166\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0125 - mae: 0.0125 - mape: 18.8066 - val_loss: 0.0126 - val_mae: 0.0126 - val_mape: 6.3283\n",
      "Epoch 26/60\n",
      "2641/2653 [============================>.] - ETA: 0s - loss: 0.0123 - mae: 0.0123 - mape: 218.4466\n",
      "Epoch 26: val_loss did not improve from 0.01166\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0123 - mae: 0.0123 - mape: 217.5152 - val_loss: 0.0120 - val_mae: 0.0120 - val_mape: 5.5404\n",
      "Epoch 27/60\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0123 - mae: 0.0123 - mape: 64.5465\n",
      "Epoch 27: val_loss did not improve from 0.01166\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0123 - mae: 0.0123 - mape: 64.5465 - val_loss: 0.0128 - val_mae: 0.0128 - val_mape: 5.7089\n",
      "Epoch 28/60\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0122 - mae: 0.0122 - mape: 167.1478\n",
      "Epoch 28: val_loss did not improve from 0.01166\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0122 - mae: 0.0122 - mape: 167.1478 - val_loss: 0.0135 - val_mae: 0.0135 - val_mape: 6.3772\n",
      "Epoch 29/60\n",
      "2633/2653 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0122 - mape: 86.8840\n",
      "Epoch 29: val_loss did not improve from 0.01166\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0122 - mae: 0.0122 - mape: 86.2756 - val_loss: 0.0136 - val_mae: 0.0136 - val_mape: 6.5306\n",
      "Epoch 30/60\n",
      "2650/2653 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0122 - mape: 295.0548\n",
      "Epoch 30: val_loss improved from 0.01166 to 0.01078, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0030-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0122 - mae: 0.0122 - mape: 294.7742 - val_loss: 0.0108 - val_mae: 0.0108 - val_mape: 5.4748\n",
      "Epoch 31/60\n",
      "2648/2653 [============================>.] - ETA: 0s - loss: 0.0121 - mae: 0.0121 - mape: 90.2817\n",
      "Epoch 31: val_loss did not improve from 0.01078\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0121 - mae: 0.0121 - mape: 90.1338 - val_loss: 0.0143 - val_mae: 0.0143 - val_mape: 6.1809\n",
      "Epoch 32/60\n",
      "2644/2653 [============================>.] - ETA: 0s - loss: 0.0120 - mae: 0.0120 - mape: 153.1856\n",
      "Epoch 32: val_loss did not improve from 0.01078\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0120 - mae: 0.0120 - mape: 152.7032 - val_loss: 0.0126 - val_mae: 0.0126 - val_mape: 5.9569\n",
      "Epoch 33/60\n",
      "2629/2653 [============================>.] - ETA: 0s - loss: 0.0120 - mae: 0.0120 - mape: 350.9715\n",
      "Epoch 33: val_loss did not improve from 0.01078\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0120 - mae: 0.0120 - mape: 347.8908 - val_loss: 0.0128 - val_mae: 0.0128 - val_mape: 6.1238\n",
      "Epoch 34/60\n",
      "2628/2653 [============================>.] - ETA: 0s - loss: 0.0119 - mae: 0.0119 - mape: 61.2666\n",
      "Epoch 34: val_loss did not improve from 0.01078\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0119 - mae: 0.0119 - mape: 60.7359 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 5.6970\n",
      "Epoch 35/60\n",
      "2634/2653 [============================>.] - ETA: 0s - loss: 0.0118 - mae: 0.0118 - mape: 231.5647\n",
      "Epoch 35: val_loss did not improve from 0.01078\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0118 - mae: 0.0118 - mape: 229.9740 - val_loss: 0.0126 - val_mae: 0.0126 - val_mape: 5.5805\n",
      "Epoch 36/60\n",
      "2647/2653 [============================>.] - ETA: 0s - loss: 0.0118 - mae: 0.0118 - mape: 165.8147\n",
      "Epoch 36: val_loss improved from 0.01078 to 0.01060, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0036-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0118 - mae: 0.0118 - mape: 165.4731 - val_loss: 0.0106 - val_mae: 0.0106 - val_mape: 5.2020\n",
      "Epoch 37/60\n",
      "2639/2653 [============================>.] - ETA: 0s - loss: 0.0117 - mae: 0.0117 - mape: 214.5919\n",
      "Epoch 37: val_loss did not improve from 0.01060\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0117 - mae: 0.0117 - mape: 213.5161 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 5.9563\n",
      "Epoch 38/60\n",
      "2648/2653 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0116 - mape: 32.5853\n",
      "Epoch 38: val_loss improved from 0.01060 to 0.01020, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0038-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0116 - mae: 0.0116 - mape: 32.5342 - val_loss: 0.0102 - val_mae: 0.0102 - val_mape: 4.8076\n",
      "Epoch 39/60\n",
      "2643/2653 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0116 - mape: 206.4061\n",
      "Epoch 39: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0116 - mae: 0.0116 - mape: 205.6749 - val_loss: 0.0129 - val_mae: 0.0129 - val_mape: 6.4057\n",
      "Epoch 40/60\n",
      "2630/2653 [============================>.] - ETA: 0s - loss: 0.0117 - mae: 0.0117 - mape: 11.8611\n",
      "Epoch 40: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0117 - mae: 0.0117 - mape: 11.7971 - val_loss: 0.0136 - val_mae: 0.0136 - val_mape: 6.2516\n",
      "Epoch 41/60\n",
      "2642/2653 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0116 - mape: 168.5172\n",
      "Epoch 41: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0116 - mae: 0.0116 - mape: 167.8629 - val_loss: 0.0136 - val_mae: 0.0136 - val_mape: 7.0316\n",
      "Epoch 42/60\n",
      "2646/2653 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0116 - mape: 113.0972\n",
      "Epoch 42: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0116 - mae: 0.0116 - mape: 112.8251 - val_loss: 0.0112 - val_mae: 0.0112 - val_mape: 4.9486\n",
      "Epoch 43/60\n",
      "2633/2653 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0116 - mape: 57.3640\n",
      "Epoch 43: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0116 - mae: 0.0116 - mape: 56.9693 - val_loss: 0.0121 - val_mae: 0.0121 - val_mape: 6.3443\n",
      "Epoch 44/60\n",
      "2645/2653 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0115 - mape: 54.8535\n",
      "Epoch 44: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0115 - mae: 0.0115 - mape: 54.7057 - val_loss: 0.0117 - val_mae: 0.0117 - val_mape: 5.2039\n",
      "Epoch 45/60\n",
      "2651/2653 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0114 - mape: 223.1935\n",
      "Epoch 45: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0114 - mae: 0.0114 - mape: 223.0635 - val_loss: 0.0117 - val_mae: 0.0117 - val_mape: 5.2406\n",
      "Epoch 46/60\n",
      "2642/2653 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0115 - mape: 27.2227\n",
      "Epoch 46: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0115 - mae: 0.0115 - mape: 27.1283 - val_loss: 0.0110 - val_mae: 0.0110 - val_mape: 5.7125\n",
      "Epoch 47/60\n",
      "2645/2653 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0115 - mape: 94.7901\n",
      "Epoch 47: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0115 - mae: 0.0115 - mape: 94.5297 - val_loss: 0.0124 - val_mae: 0.0124 - val_mape: 5.6632\n",
      "Epoch 48/60\n",
      "2649/2653 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0113 - mape: 272.9592\n",
      "Epoch 48: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0113 - mae: 0.0113 - mape: 272.5976 - val_loss: 0.0103 - val_mae: 0.0103 - val_mape: 4.9847\n",
      "Epoch 49/60\n",
      "2639/2653 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0114 - mape: 254.5736\n",
      "Epoch 49: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0114 - mae: 0.0114 - mape: 253.2889 - val_loss: 0.0109 - val_mae: 0.0109 - val_mape: 4.9314\n",
      "Epoch 50/60\n",
      "2630/2653 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0113 - mape: 145.2252\n",
      "Epoch 50: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0113 - mae: 0.0113 - mape: 144.0211 - val_loss: 0.0138 - val_mae: 0.0138 - val_mape: 7.4605\n",
      "Epoch 51/60\n",
      "2648/2653 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0112 - mape: 128.8218\n",
      "Epoch 51: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0113 - mae: 0.0113 - mape: 128.6068 - val_loss: 0.0106 - val_mae: 0.0106 - val_mape: 5.3755\n",
      "Epoch 52/60\n",
      "2649/2653 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0113 - mape: 221.8443\n",
      "Epoch 52: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0113 - mae: 0.0113 - mape: 221.5524 - val_loss: 0.0109 - val_mae: 0.0109 - val_mape: 4.8921\n",
      "Epoch 53/60\n",
      "2647/2653 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0112 - mape: 225.7097\n",
      "Epoch 53: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0112 - mae: 0.0112 - mape: 225.2431 - val_loss: 0.0105 - val_mae: 0.0105 - val_mape: 5.5690\n",
      "Epoch 54/60\n",
      "2645/2653 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0112 - mape: 7.0608\n",
      "Epoch 54: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0112 - mae: 0.0112 - mape: 7.0510 - val_loss: 0.0123 - val_mae: 0.0123 - val_mape: 5.4172\n",
      "Epoch 55/60\n",
      "2642/2653 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0113 - mape: 430.5050\n",
      "Epoch 55: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0113 - mae: 0.0113 - mape: 428.8050 - val_loss: 0.0107 - val_mae: 0.0107 - val_mape: 4.9730\n",
      "Epoch 56/60\n",
      "2626/2653 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0111 - mape: 278.5728\n",
      "Epoch 56: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0110 - mae: 0.0110 - mape: 275.8179 - val_loss: 0.0112 - val_mae: 0.0112 - val_mape: 5.8893\n",
      "Epoch 57/60\n",
      "2638/2653 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0112 - mape: 217.3573\n",
      "Epoch 57: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0112 - mae: 0.0112 - mape: 216.1831 - val_loss: 0.0108 - val_mae: 0.0108 - val_mape: 5.1725\n",
      "Epoch 58/60\n",
      "2649/2653 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0113 - mape: 223.3244\n",
      "Epoch 58: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0113 - mae: 0.0113 - mape: 223.0290 - val_loss: 0.0112 - val_mae: 0.0112 - val_mape: 5.2257\n",
      "Epoch 59/60\n",
      "2637/2653 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0111 - mape: 3.7523\n",
      "Epoch 59: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0111 - mae: 0.0111 - mape: 3.7488 - val_loss: 0.0105 - val_mae: 0.0105 - val_mape: 4.8332\n",
      "Epoch 60/60\n",
      "2637/2653 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0111 - mape: 183.1444\n",
      "Epoch 60: val_loss did not improve from 0.01020\n",
      "2653/2653 [==============================] - 7s 3ms/step - loss: 0.0111 - mae: 0.0111 - mape: 182.1286 - val_loss: 0.0116 - val_mae: 0.0116 - val_mape: 5.5965\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                    verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 1s 1ms/step\n",
      "Mean Absolute Error (MAE): 164.07\n",
      "Median Absolute Error (MedAE): 133.39\n",
      "Mean Squared Error (MSE): 44254.42\n",
      "Root Mean Squared Error (RMSE): 210.37\n",
      "Mean Absolute Percentage Error (MAPE): 1.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.93 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(r'C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0038-loss0.01.h5')\n",
    "\n",
    "y_pred_scaled   = model.predict(test_X)\n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled)) \n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = r'C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model=r'C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0038-loss0.01.h5'\n",
    "start_epoch= 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E1-cp-0038-loss0.01.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]\n",
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=24, num_features=21, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2625/2653 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0091 - mape: 94.3988\n",
      "Epoch 1: val_loss improved from inf to 0.00950, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab2\\E2-cp-0001-loss0.01.h5\n",
      "2653/2653 [==============================] - 7s 2ms/step - loss: 0.0091 - mae: 0.0091 - mape: 93.4461 - val_loss: 0.0095 - val_mae: 0.0095 - val_mape: 4.6159\n",
      "Epoch 2/10\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0090 - mae: 0.0090 - mape: 45.7124\n",
      "Epoch 2: val_loss improved from 0.00950 to 0.00920, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab2\\E2-cp-0002-loss0.01.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 45.7124 - val_loss: 0.0092 - val_mae: 0.0092 - val_mape: 4.3533\n",
      "Epoch 3/10\n",
      "2632/2653 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0090 - mape: 53.8044\n",
      "Epoch 3: val_loss did not improve from 0.00920\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 53.4095 - val_loss: 0.0095 - val_mae: 0.0095 - val_mape: 4.4109\n",
      "Epoch 4/10\n",
      "2632/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 6.2736\n",
      "Epoch 4: val_loss did not improve from 0.00920\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 6.2483 - val_loss: 0.0093 - val_mae: 0.0093 - val_mape: 4.2993\n",
      "Epoch 5/10\n",
      "2646/2653 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0090 - mape: 4.1870\n",
      "Epoch 5: val_loss did not improve from 0.00920\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0090 - mae: 0.0090 - mape: 4.1830 - val_loss: 0.0092 - val_mae: 0.0092 - val_mape: 4.4293\n",
      "Epoch 6/10\n",
      "2631/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 40.0921\n",
      "Epoch 6: val_loss improved from 0.00920 to 0.00898, saving model to C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab2\\E2-cp-0006-loss0.01.h5\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 39.7903 - val_loss: 0.0090 - val_mae: 0.0090 - val_mape: 4.0811\n",
      "Epoch 7/10\n",
      "2628/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 51.3619\n",
      "Epoch 7: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 50.9114 - val_loss: 0.0092 - val_mae: 0.0092 - val_mape: 4.2066\n",
      "Epoch 8/10\n",
      "2643/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 54.4526\n",
      "Epoch 8: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 54.2669 - val_loss: 0.0096 - val_mae: 0.0096 - val_mape: 4.3860\n",
      "Epoch 9/10\n",
      "2638/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 35.8424\n",
      "Epoch 9: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 35.6599 - val_loss: 0.0091 - val_mae: 0.0091 - val_mape: 4.1363\n",
      "Epoch 10/10\n",
      "2628/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 29.1699\n",
      "Epoch 10: val_loss did not improve from 0.00898\n",
      "2653/2653 [==============================] - 6s 2ms/step - loss: 0.0089 - mae: 0.0089 - mape: 28.9249 - val_loss: 0.0096 - val_mae: 0.0096 - val_mape: 4.3660\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 1s 1ms/step\n",
      "Mean Absolute Error (MAE): 144.75\n",
      "Median Absolute Error (MedAE): 113.65\n",
      "Mean Squared Error (MSE): 36189.68\n",
      "Root Mean Squared Error (RMSE): 190.24\n",
      "Mean Absolute Percentage Error (MAPE): 0.98 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.79 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(r'C:\\Users\\Administrator\\Downloads\\ML Lab\\checkpoint\\ML Lab\\lab7\\E2-cp-0006-loss0.01.h5')\n",
    "\n",
    "y_pred_scaled   = model.predict(test_X)\n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled)) \n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
